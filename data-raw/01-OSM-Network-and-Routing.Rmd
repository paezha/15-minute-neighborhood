---
title: "Network and Routing"
author: "Antonio Paez"
date: "`r Sys.Date()`"
output: html_notebook
---

**NOTE:** Notebook takes about 6 hours to run.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this document I collect and preprocess the network data for this research.

# Preliminaries

Load packages:
```{r warning=FALSE, message=FALSE}
# Allocate memory for routing
options(java.parameters = "-Xmx20G")

library(disk.frame) # Larger-than-RAM Disk-Based Data Manipulation Framework
library(dplyr) # A Grammar of Data Manipulation
library(ggforce) # Accelerating 'ggplot2'
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(h3) # R Bindings for H3
library(purrr) # Functional Programming Tools
library(r5r) # Rapid Realistic Routing with 'R5'
library(sf) # Simple Features for R
library(skimr) # Compact and Flexible Summaries of Data
library(units) # Measurement Units for R Vectors
library(vaccHamilton) # A Data Package to Estimate Accessibility of Vaccination Sites in Hamilton, ON
```

Load data objects from package {vaccHamilton}. Urban types is the way the City of Hamilton classifies different parts of the city as "Urban", "Suburban", and "Rural". The second object (`data_da_2016`) includes the geometry of Dissemination Areas and some demographic data for 2016:
```{r}
data("urban_types")
data("data_da_2016")
```

Use only urban and suburban Hamilton:
```{r}
urban_hammer <- urban_types |>
  filter(Type != "Rural")
```

Find the DAs that are urban or suburban:
```{r}
urban_hammer_da <- data_da_2016 |>
  st_transform(crs = st_crs(urban_types)) |>
  st_intersection(urban_hammer)
```

Search for duplicates after the intersection:
```{r}
duplicate_DAs <- urban_hammer_da |> 
  st_drop_geometry() |>
  group_by(GeoUID) |> 
  summarize(n = n()) |>
  filter(n > 1)
```

Duplicates are caused by small inconsistencies between the DA boundaries and the urban/suburban boundaries, and the partitions result in slivers of DA:
```{r}
urban_hammer_da |>
  filter(GeoUID %in% duplicate_DAs$GeoUID[sample.int(50, 1)]) |>
  ggplot() +
  geom_sf(aes(fill = GeoUID))
```

This leads to some detritus:
```{r}
urban_hammer_da |>
  filter(GeoUID == "35250031") |>
  ggplot() +
  geom_sf(aes(fill = GeoUID)) +
  geom_sf(data = data_da_2016,
          fill = NA)
```

Summary of areas of the areas of DAs after the intersection. `r` is the area of the zone after the intersection divided by the area of the original DA:
```{r}
urban_hammer_da <- urban_hammer_da |>
  mutate(area = st_area(geometry) |> set_units(km^2)) |>
    mutate(r = area/Shape.Area)

urban_hammer_da |>
  filter(GeoUID %in% duplicate_DAs$GeoUID) |>
  select(GeoUID, Shape.Area, Area..sq.km., area, r) |>
  summary()
```

This is the distribution of the areas of all duplicated DAs
```{r}
urban_hammer_da |>
  filter(GeoUID %in% duplicate_DAs$GeoUID) |>
  ggplot(aes(x = r)) +
  geom_histogram()
```

These are them:
```{r}
ggplot() +
  geom_sf(data = urban_hammer_da |>
            filter(GeoUID %in% duplicate_DAs$GeoUID),
          aes(fill = drop_units(r))) +
  geom_sf(data = data_da_2016,
          fill = NA)
```

Split the table into DAs that had no duplicates and DAs that did:
```{r}
urban_hammer_dup <- urban_hammer_da |>
  st_transform(crs = st_crs(data_da_2016)) |>
  filter(GeoUID %in% duplicate_DAs$GeoUID)

urban_hammer_nondup <- urban_hammer_da |>
  st_transform(crs = st_crs(data_da_2016)) |>
  filter(!(GeoUID %in% duplicate_DAs$GeoUID))
```

Choose areas to keep that are not detritus (areas that are more than 50% of the area of the original DA):
```{r}
urban_hammer_dup <- urban_hammer_dup |>
  filter(r > set_units(0.50, km^2))
```

Replace the geometry by its original geometry:
```{r}
urban_hammer_dup <- urban_hammer_dup |>
  st_drop_geometry() |>
  left_join(data_da_2016 |>
              select(GeoUID),
            by = "GeoUID") |>
  select(-c(area, r)) |>st_as_sf()
```

Bind with non-duplicated DAs (who get to keep the geometry they got after intersecting with the urban types):
```{r}
urban_hammer_da <- rbind(urban_hammer_nondup |>
                           select(-c(area, r)),
                         urban_hammer_dup)
```

Plot urban/suburban DAs:
```{r}
ggplot() + 
  geom_sf(data = urban_hammer_da) + 
  geom_sf(data = urban_hammer,
          aes(color = Type), 
          fill = NA, 
          linewidth = 1)
```

Coordinates of centroids of the DAs that intersect urban/suburban types (in lat-long). These will be the origins of trips, that is, our proxies for "neighborhoods":
```{r}
origins_da <- st_centroid(urban_hammer_da) |>
  st_transform(crs = st_crs(4326))
```

Also, sample in urban and suburban zones to replicate the number of points for DAs. We will this as a validation dataset.
```{r}
# For reproducibility
seed <- 22331264
set.seed(seed)

origins_validation_suburban <- st_sample(urban_hammer_da |>
            filter(Type.1 == "Suburban"),
          size = rep(1, nrow(urban_hammer_da |>
            filter(Type.1 == "Suburban")))) |>
  st_as_sf() |>
  mutate(Type = "Suburban")

origins_validation_urban <- st_sample(urban_hammer_da |>
            filter(Type.1 == "Urban"),
          size = rep(1, nrow(urban_hammer_da |>
            filter(Type.1 == "Urban")))) |>
  st_as_sf() |>
  mutate(Type = "Urban")
```

Bind and plot:
```{r}
origins_validation <- rbind(origins_validation_suburban,
                          origins_validation_urban) |>
  st_join(data_da_2016 |>
            select(GeoUID)) |>
  mutate(geometry = x)

ggplot() +
  geom_sf(data = urban_types) + 
  geom_sf(data = origins_validation,
          aes(color = Type))
```

# Geohashes

The simplest way to proceed, other than trying to calculate the travel times to every single amenity (and considering that we will at some point simulate the locations of amenities) is to geohash the region of interest. That way, we can count the number of amenities for polygons with precalculated travel times.

Retrieve h3 geohashing at a fairly high resolution to identify Hamilton; the coordinates are for the City of Hamilton:
```{r}
urban_hammer_h3 <- geo_to_h3(c(43.2501, -79.8496),
                             res = 4) |>
  k_ring(radius = 1)
```

Plot the geohash retrieved in this way:
```{r}
ggplot() + 
  geom_sf(data = h3_to_geo_boundary_sf(urban_hammer_h3),
          aes(fill = h3_index)) +
  geom_sf(data = urban_types,
          fill = NA)
```

Keep only the h3 indices that overlap with Hamilton:
```{r}
urban_hammer_h3 = c("842b9b7ffffffff", "842b9b5ffffffff", "842ab4bffffffff")
```

Plot kept indices:
```{r}
ggplot() + 
  geom_sf(data = h3_to_geo_boundary_sf(urban_hammer_h3),
          aes(fill = h3_index)) +
  geom_sf(data = urban_types,
          fill = NA)
```

Obtain children at a higher resolution:
```{r}
urban_hammer_h3 <- urban_hammer_h3 |> 
  purrr::map(\(x) h3_to_children(x, res = 8)) |>
  unlist()
```

Plot current geohashes:
```{r}
ggplot() + 
  geom_sf(data = h3_to_geo_boundary_sf(urban_hammer_h3)) +
  geom_sf(data = urban_types,
          fill = NA)
```

Choose only hexagons that touch urban/suburban Hamilton:
```{r}
urban_hammer_h3 <- h3_to_geo_boundary_sf(urban_hammer_h3) |>
  st_transform(crs = st_crs(urban_types)) |>
  st_intersection(urban_types |> 
                    filter(Type != "Rural"))

urban_hammer_h3 <- urban_hammer_h3 |> 
  pull(h3_index)
```

Plot:
```{r}
ggplot() + 
  geom_sf(data = h3_to_geo_boundary_sf(urban_hammer_h3)) +
  geom_sf(data = urban_types,
          fill = NA)
```

Currently this is resolution 8:
```{r}
h3_get_resolution(urban_hammer_h3)
```

Obtain children at resolution 13 (average area is 43.87 $m^2$):
```{r}
urban_hammer_h3 <- urban_hammer_h3 |> 
  purrr::map(\(x) h3_to_children(x, res = 13)) |>
  unlist()
```

Convert to coordinates and store to use as destinations:
```{r}
destinations_h3 <- h3_to_geo(urban_hammer_h3)
```

## Network routing

I used [BBBike](https://download.bbbike.org/osm/bbbike/) to extract OSM data for Hamilton on July 7, 2023. The name of the file is `planet_-80.279,43.048_-79.318,43.459.osm.pbf`. I copied this file to folder `data-raw/r5_graph`.

Set Up R5 Routing. First define the path to where the graph is located:
```{r set up r5 path, include=FALSE}
r5_path <- file.path("./r5_graph")
```

Download and import GTFS (Hamilton transit data)
```{r eval=FALSE}
download.file(url = "https://transitfeeds.com/p/hamilton-street-railway/31/latest/download", 
              destfile = file.path(r5_path, 
                                   "HSR_transit.zip"), 
              mode = "wb")
```

Download and import GTFS (Hamilton transit data)
```{r eval=FALSE}
download.file(url = "https://transitfeeds.com/p/burlington-transit/294/latest/download", 
              destfile = file.path(r5_path, 
                                   "Burlington_transit.zip"), 
              mode = "wb")
```

Build the graph:
```{r build-graph, include = FALSE}
r5_hamilton_cma <- setup_r5(data_path = r5_path, 
                            verbose = FALSE)
```

Retrieve network as a simple features object:
```{r}
hamilton_net <- street_network_to_sf(r5_hamilton_cma)
```

Save network as sf:
```{r}
save(hamilton_net, file = "hamilton_net.rda", compress = "xz")
```

Prepare Input Data for `r5r` for the DAs. 

The origins are the coordinates of the DAs and the destinations the coordinates of the h3 geohashes:
```{r prepare-inputs}
# save origins in format expected by R5R (id, lon, lat)
origins_i_da <- data.frame(ID = origins_da$GeoUID, 
                        origins_da |>
                          st_transform(crs = 4326) |>
                          st_coordinates()) |>
  rename(lon = X, lat = Y, id = ID) |>
  dplyr::select(id, lon, lat)

# save origins in format expected by R5R (id, lon, lat)
origins_i_validation <- data.frame(ID = origins_validation$GeoUID, 
                        origins_validation |>
                          st_transform(crs = 4326) |>
                          st_coordinates()) |>
  rename(lon = X, lat = Y, id = ID) |>
  dplyr::select(id, lon, lat)

# now destinations sites
destinations_j <- data.frame(id = urban_hammer_h3, 
                             lat = destinations_h3[,1],
                             lon = destinations_h3[,2])
```

Calculate OD Matrix for walking (DAs):
```{r calculate walk od matrix DAs}

# set up batching according to how many origin rows to process at one time
chunksize = 4000 # larger chunks for walking will give enough origins in each chunk to allow multiprocessing to spin up with R5R
num_chunks = ceiling(nrow(origins_i_da)/chunksize)

# create origin-destination pairs
origins_chunks <- as.disk.frame(origins_i_da,
                                outdir = "./df/origins_i_da",
                                nchunks = num_chunks,
                                overwrite = TRUE)

start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)

for (i in 1:num_chunks){ 
  origins_i_chunk <- get_chunk(origins_chunks, i)
  ttm_chunk <- travel_time_matrix(r5_hamilton_cma,
                                  origins = origins_i_chunk,
                                  destinations = destinations_j,
                                  mode = c("WALK"),
                                  departure_datetime = as.POSIXct(strptime("2021-04-05 08:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST5EDT")),
                                  max_walk_time = 15, # minutes
                                  max_trip_duration = 15)
  
  # export output as disk.frame
  ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
                                            nchunks = 1,
                                            outdir = "./df/output_ttm_walk_da",
                                            compress = 50,
                                            overwrite = TRUE),
         add_chunk(output_df, ttm_chunk, chunk_id = i))
  setTxtProgressBar(pb, i)
}

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

Calculate OD Matrix for walking (validation points):
```{r calculate walk od matrix validation}

# set up batching according to how many origin rows to process at one time
chunksize = 4000 # larger chunks for walking will give enough origins in each chunk to allow multiprocessing to spin up with R5R
num_chunks = ceiling(nrow(origins_i_validation)/chunksize)

# create origin-destination pairs
origins_chunks <- as.disk.frame(origins_i_validation,
                                outdir = "./df/origins_i_validation",
                                nchunks = num_chunks,
                                overwrite = TRUE)

start.time <- Sys.time()
pb <- txtProgressBar(0, num_chunks, style = 3)

for (i in 1:num_chunks){ 
  origins_i_chunk <- get_chunk(origins_chunks, i)
  ttm_chunk <- travel_time_matrix(r5_hamilton_cma,
                                  origins = origins_i_chunk,
                                  destinations = destinations_j,
                                  mode = c("WALK"),
                                  departure_datetime = as.POSIXct(strptime("2021-04-05 08:00:00", "%Y-%m-%d %H:%M:%S", tz = "EST5EDT")),
                                  max_walk_time = 15, # minutes
                                  max_trip_duration = 15)
  
  # export output as disk.frame
  ifelse(i == 1, output_df <- as.disk.frame(ttm_chunk,
                                            nchunks = 1,
                                            outdir = "./df/output_ttm_walk_validation",
                                            compress = 50,
                                            overwrite = TRUE),
         add_chunk(output_df, ttm_chunk, chunk_id = i))
  setTxtProgressBar(pb, i)
}

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

## Extract travel time matrices

For DAs:
```{r load od matrix for walk DAs, include=FALSE}
# connect to the walking travel time matrix disk frame
ttm_walk.disk.frame <- disk.frame("./df/output_ttm_walk_da")
#opportunities_j <- as.data.frame(nyc_cb_point) |> select(GEOID10, total_emp) |> rename(toId = GEOID10, o_j = total_emp)
```

Convert disk.frame to data frame:
```{r}
ttm_walk_da <- as.data.frame(ttm_walk.disk.frame) |>
  transmute(GeoUID = from_id, h3_index = to_id, travel_time = travel_time_p50)
```

For validation points:
```{r load od matrix for walk validation points, include=FALSE}
# connect to the walking travel time matrix disk frame
ttm_walk.disk.frame <- disk.frame("./df/output_ttm_walk_validation")
#opportunities_j <- as.data.frame(nyc_cb_point) |> select(GEOID10, total_emp) |> rename(toId = GEOID10, o_j = total_emp)
```

Convert disk.frame to data frame:
```{r}
ttm_walk_validation <- as.data.frame(ttm_walk.disk.frame) |>
  transmute(GeoUID = from_id, h3_index = to_id, travel_time = travel_time_p50)
```

## Retrieve h3 geometries of walksheds by origin

For DAs: retrieve the h3 areas as multipolygons:
```{r}
start.time <- Sys.time()

walksheds_da <- ttm_walk_da |> 
  group_by(GeoUID) |>
  group_map(~ h3_set_to_multi_polygon(.x$h3_index),
            .keep = TRUE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

Obtain the GeoUIDs of the walksheds:
```{r}
walkshed_GeoUIDs_da <- ttm_walk_da |>
  group_by(GeoUID) |>
  summarize(GeoUID = first(GeoUID),
            .groups = "drop")
```

Convert walksheds with GeoUIDs of DAs to a data frame:
```{r}
start.time <- Sys.time()

walksheds_da <- data.frame(GeoUID = walkshed_GeoUIDs_da,
                         geometry = purrr::map_df(walksheds_da,
                            tibble::as_tibble)) |>
  st_as_sf(crs = 4326)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

For validation points: retrieve the h3 areas as multipolygons:
```{r}
start.time <- Sys.time()

walksheds_validation <- ttm_walk_validation |> 
  group_by(GeoUID) |>
  group_map(~ h3_set_to_multi_polygon(.x$h3_index),
            .keep = TRUE)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

Obtain the GeoUIDs of the walksheds:
```{r}
walkshed_GeoUIDs_validation <- ttm_walk_validation |>
  group_by(GeoUID) |>
  summarize(GeoUID = first(GeoUID),
            .groups = "drop")
```

Convert walksheds with GeoUIDs of DAs to a data frame:
```{r}
start.time <- Sys.time()

walksheds_validation <- data.frame(GeoUID = walkshed_GeoUIDs_validation,
                         geometry = purrr::map_df(walksheds_validation,
                            tibble::as_tibble)) |>
  st_as_sf(crs = 4326)

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

Verify the catchments:
```{r}
test_unit = sample.int(nrow(walksheds_da), 1)

ggplot() +
  geom_sf(data = data_da_2016 |>
            filter(GeoUID == walksheds_da$GeoUID[test_unit]),
          color = "black",
          fill= NA) + 
  geom_sf(data = walksheds_da[test_unit,],
          color = "blue",
          fill = NA,
          linewidth = 1)
```

```{r}
test_unit = sample.int(nrow(walksheds_validation), 1)

ggplot() +
  geom_sf(data = data_da_2016 |>
            filter(GeoUID == walksheds_validation$GeoUID[test_unit]),
          color = "black",
          fill= NA) + 
  geom_sf(data = walksheds_validation[test_unit,],
          color = "blue",
          fill = NA,
          linewidth = 1)
```

Join the urban type to DAs in the walksheds_da object:
```{r}
walksheds_da <- walksheds_da |>
  left_join(origins_da |>
              st_drop_geometry() |>
              transmute(GeoUID, Type = Type.1),
            by = "GeoUID")
```

Join the urban type to DAs in the walksheds_validation object:
```{r}
walksheds_validation <- walksheds_validation |>
  left_join(origins_validation |>
              st_drop_geometry() |>
              transmute(GeoUID, Type = Type),
            by = "GeoUID")
```

Convert to factor:
```{r}
walksheds_da <- walksheds_da |>
  mutate(Type = factor(Type))

walksheds_validation <- walksheds_validation |>
  mutate(Type = factor(Type))
```

Summarize the walksheds:
```{r}
summary(walksheds_da)

summary(walksheds_validation)
```

The DAs and the catchments match. Now save the data objects:
```{r}
save(urban_hammer_da, file = "urban_hammer_da.rda", compress = "xz")
save(ttm_walk_da, file = "ttm_walk_da.rda", compress = "xz")
save(walksheds_da, file = "walksheds_da.rda", compress = "xz")
save(ttm_walk_validation, file = "ttm_walk_validation.rda", compress = "xz")
save(walksheds_validation, file = "walksheds_validation.rda", compress = "xz")
```

Calculate area of walksheds:
```{r}
walksheds_da <- walksheds_da |>
  mutate(area = st_area(geometry))

walksheds_validation <- walksheds_validation |>
  mutate(area = st_area(geometry))
```

Summarize DA walksheds by urban type:
```{r}
walksheds_da |>
  st_drop_geometry() |>
  mutate(area = units::drop_units(area)) |>
  group_by(Type) |>
  skim()
```

Summarize validation walksheds by urban type:
```{r}
walksheds_validation |>
  st_drop_geometry() |>
  mutate(area = units::drop_units(area)) |>
  group_by(Type) |>
  skim()
```

Linear model of walksheds area as a function of urban type:
```{r}
walksheds_da |>
  st_drop_geometry() |>
  mutate(area = log(units::drop_units(set_units(area, km2)))) |>
  lm(area ~ Type, data = _) |>
  summary()
```

Linear model of walksheds area as a function of urban type:
```{r}
walksheds_validation |>
  st_drop_geometry() |>
  mutate(area = log(units::drop_units(set_units(area, km2)))) |>
  lm(area ~ Type, data = _) |>
  summary()
```


```{r}
ggplot(data = rbind(walksheds_da |>
                      mutate(source = "DAs"),
                    walksheds_validation |>
                      mutate(source = "validation")) |>
  st_drop_geometry() |>
  mutate(area = units::drop_units(set_units(area, km2))),
  aes(x = Type, y = area)) +
  geom_violin() +
  facet_wrap(~ source)
```

